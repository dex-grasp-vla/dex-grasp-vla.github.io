<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping</title>

    <!-- 添加favicon -->
    <link rel="icon" type="image/x-icon" href="assets/images/favicon.jpg">
    <!-- 或者使用PNG格式 -->
    <!-- <link rel="icon" type="image/png" href="assets/favicon.png"> -->

    <script>
        var task_map = {
            "simple-object-manipulation": "simple_object_manipulation",
            "visual-goal-reaching": "visual_goal_reaching",
            "novel-concept-grounding": "novel_concept_grounding",
            "one-shot-video-imitation": "one_shot_video_imitation",
            "visual-constraint-satisfaction": "visual_constraint_satisfaction",
            "visual-reasoning": "visual_reasoning"
        };

        function updateDemoVideo(category) {
            // var demo = document.getElementById("single-menu-demos").value;
            var task = document.getElementById(category + "-menu-tasks").value;
            var inst = document.getElementById(category + "-menu-instances").value;

            console.log(task_map[category], task, inst)

            var video = document.getElementById(category + "-single-task-video");
            video.src = "assets/videos/demos/" +
                task_map[category] +
                "/" +
                task +
                "/" +
                inst +
                ".mp4";
            video.playbackRate = 2.0;
            video.play();
        }
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <style>
      .content.has-text-justified p,
      .content.has-text-justified ul,
      .content.has-text-justified ol {
          font-size: 120%;  /* 可以调整这个百分比来改变大小 */
      }

      .content.has-text-justified li {
        font-size: inherit;  /* 继承父元素的字体大小 */
      }
      
      .hover-card p,
      .hover-card ul,
      .hover-card ol {
          font-size: 120%;
      }
      .hover-card li {
        font-size: inherit;
    }
  </style>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-2 publication-title">DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping</h1>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a target="_blank" href="https://github.com/dex-grasp-vla/dex-grasp-vla"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a target="_blank" href="https://www.youtube.com/watch?v=yHFjyQ0TqQw"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <span class="link-block">
                <a target="_blank" href="https://drive.google.com/file/d/18F5gUAhjfwaJ-6wLxhnVKififuioFT4k/view?usp=drive_link"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-google"></i>
                  </span>
                  <span>Model</span>
                </a>
              </span>
            </div>
          </div>
        </div>
    </div>
</section>

<section class="hero teaser">
  <div class="container">
    <div class="hero-body">
      <!-- Paper video -->
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <div class="publication-video" style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; margin-bottom: 20px; width: 900px; margin-left: auto; margin-right: auto;">
            <iframe 
              style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"
              src="https://www.youtube.com/embed/yHFjyQ0TqQw"
              frameborder="0" 
              allow="autoplay; encrypted-media" 
              allowfullscreen>
            </iframe>
          </div>
        </div>
      </div>
      <!-- /Paper video -->
  </div>
</section>

<section class="hero is-light">
  <div class="hero-body">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p style="font-size: 125%">
            Dexterous grasping remains a fundamental yet challenging problem in robotics. A general-purpose robot must be capable of grasping diverse objects in arbitrary scenarios. However, existing research typically relies on restrictive assumptions, such as single-object settings or limited environments, leading to constrained generalization. We present DexGraspVLA, a hierarchical framework for general dexterous grasping in cluttered scenes based on RGB image perception and language instructions. It utilizes a pre-trained Vision-Language model as the high-level task planner and learns a diffusion-based policy as the low-level Action controller. The key insight to achieve robust generalization lies in iteratively transforming diverse language and visual inputs into domain-invariant representations via foundation models, where imitation learning can be effectively applied due to the alleviation of domain shift. Notably, our method achieves a 90+% success rate under thousands of unseen object, lighting, and background combinations in a “zero-shot” environment. Empirical analysis confirms the consistency of internal model behavior across environmental variations, thereby validating our design and explaining its generalization performance. DexGraspVLA also demonstrates free-form long-horizon prompt execution, robustness to adversarial objects and human disturbance, and failure recovery, which are rarely achieved simultaneously in prior work. Extended application to nonprehensile object grasping further proves its generality.
          </p>
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <img src="assets/images/method.jpg" class="interpolation-image"
               alt="" style="display: block; margin-left: auto; margin-right: auto" width="900"/>
               <div class="content has-text-justified">
                <div class="content has-text-justified hover-card" style="padding: 1.5rem;">
                  <p>DexGraspVLA adopts a hierarchical architecture composed of an off-the-shelf VLM-based high-level 
                  <span style="color:#6a7bff; font-weight:bold;">planner</span> and a diffusion-based low-level <span style="color:#ff6b6b; font-weight:bold;">controller</span>. 
                  Given a cluttered scene, the <span style="color:#6a7bff; font-weight:bold;">planner</span> grounds the user prompt, e.g., <i>"clear the table"</i>, in the observation and proposes grasping instructions \(\{l_i\}\) sequentially.
                  For each instruction \(l\), e.g., <i>"grasp the cookie"</i>, the <span style="color:#6a7bff; font-weight:bold;">planner</span> identifies the target object \(A\) from the head image \(\mathbf{I}_{t_0}\) 
                  and marks its bounding box \((x_1^A, y_1^A, x_2^A, y_2^A)\) at initial time \(t_0\).</p>
        
                  <p>The <span style="color:#ff6b6b; font-weight:bold;">controller</span> consists of four parts:</p>
                  <ol>
                    <li>Two segmentation models including SAM, which obtains the object's mask \(\mathbf{m}_{t_0}\) at \(t_0\), and Cutie, a video segmentation model that continuously tracks the mask \(\mathbf{m}_t\) during each grasping process.</li>
                    <li>Three vision encoders including two frozen DINOv2 that extract features from the third-view head-camera image \(\mathbf{I}_t^h\) and the first-view wrist-camera image \(\mathbf{I}_t^w\), and a trainable ViT that deals with the mask \(\mathbf{m}_t\).</li>
                    <li>Three MLP projectors that map the visual features and robot proprioceptive state into the same feature space, forming a feature sequence.</li>
                    <li>A DiT that predicts an action chunk from \(\mathbf{a}_t\) to \(\mathbf{a}_{t+H-1}\).</li>
                  </ol>
        
                  <p>During the <span style="color:#ff6b6b; font-weight:bold;">controller</span>'s grasping process, the <span style="color:#6a7bff; font-weight:bold;">planner</span> monitors the execution and triggers a scripted placing motion when grasping succeeds. After each grasping attempt, the <span style="color:#6a7bff; font-weight:bold;">planner</span> resets the robot and proposes a new grasping instruction. This process continues until the user prompt is fully completed.</p>
                  <p>The <span style="color:#ff6b6b; font-weight:bold;">controller</span> is trained on a dataset consisting of 2,094 successful grasping episodes in cluttered scenes. These demonstrations are collected at typical human motion speeds, with each episode taking approximately 3.5 seconds. In total, this amounts to roughly two hours of data.</p>
                </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-light">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <!-- Generalization Videos -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Experiments</h2>
          <h3 class="title is-4"><span
            class="dvima">Large-Scale Generalization Evaluation
          </span></h3>
          <div class="content has-text-justified">
            <img src="./assets/images/experiment-setup.jpg" class="interpolation-image"
                 alt="" style="display: block; margin-left: auto; margin-right: auto" width="900"/>
                  <div class="content has-text-justified hover-card" style="padding: 1.5rem;">
                      <p>
                        To thoroughly evaluate the <strong>generalization performance</strong> of DexGraspVLA, we curate 
                        (a) <strong>360 unseen objects</strong> covering a diverse range of sizes, weights, geometries, textures, materials, and categories, 
                        and (b) <strong>3 unseen lighting conditions</strong> and <strong>6 unseen backgrounds</strong> that significantly differ from the training data.
                    </p>
                    <p>
                        Based on this setup, we design <strong>three types of grasping tasks</strong> in <strong>cluttered scenes</strong>, with each scene containing approximately six objects. 
                        The tasks involve grasping an unseen object from a random scene under the following conditions:
                    </p>
                    <ul>
                        <li><strong>Unseen Objects</strong>: on a white table under white light.</li>
                        <li><strong>Unseen Backgrounds</strong>: on an unseen background under white light.</li>
                        <li><strong>Unseen Lightings</strong>: on a white table under an unseen lighting condition.</li>
                    </ul>
                    <p>
                        In total, we generate <strong>over 1,200 unseen cluttered scenes</strong> in a <strong>zero-shot environment</strong> to rigorously test our method.
                    </p>
                    
                  </div>
                  <img src="./assets/images/generalization.jpg" class="interpolation-image"
                 alt="" style="display: block; margin-left: auto; margin-right: auto" width="600"/>
                 <div class="content has-text-justified hover-card" style="padding: 1.5rem;">
                    <p>
                      The table above presents the success rate of DexGraspVLA, where "Ours@\(k\)" denotes its performance when each test is given \(k\) attempts. DexGraspVLA achieves a <strong>91.1% single-attempt success rate</strong> on <i>Unseen Objects</i>, 
                      <strong>90.5%</strong> on <i>Unseen Backgrounds</i>, and <strong>90.9%</strong> on <i>Unseen Lightings</i>,
                      with an overall success rate of <strong>90.8%</strong>. These results demonstrate its ability to grasp specified 
                      objects from clutter while remaining robust to environmental changes—all without domain-specific fine-tuning.
                  </p>
                  <p>
                      Notably, despite operating in unseen environments and tasks, DexGraspVLA maintains high success rates, 
                      addressing a key challenge in imitation learning: overfitting to a single domain. It dexterously adjusts 
                      to varying object geometries, sizes, and positions, and its closed-loop policy enables re-grasps, 
                      improving robustness. The method also withstands human-induced perturbations, tracking and securing moving 
                      objects until a successful grasp is achieved.
                  </p>
                  <p>
                      With <strong>multiple attempts</strong>, performance further improves—<strong>reaching 96.9% success within three tries</strong>. 
                      Additionally, our model takes <strong>~6 seconds per grasp</strong>, comparable to human efficiency, ensuring practical 
                      real-world usability.
                  </p>
                </div>
      </div>
      <h3 class="title is-4"><span
        class="dvima">Comparison to Baselines on A Single-Object Grasping Benchmark
      </span></h3>
      <div class="content has-text-justified">
              <img src="./assets/images/single-object.jpg" class="interpolation-image"
             alt="" style="display: block; margin-left: auto; margin-right: auto" width="600"/>
             <div class="content has-text-justified hover-card" style="padding: 1.5rem;">
                <p>
                    To compare DexGraspVLA with baselines whose controllers learn directly from raw visual inputs, we conduct 
                    <strong>single-object grasping experiments</strong> on <strong>13 seen objects</strong> and <strong>8 unseen objects</strong>. 
                    Each object is placed at <strong>five different table locations</strong>, ensuring coverage of the workspace and the robot's reach. 
                    At each location, the policy attempts <strong>two independent grasps</strong>, totaling <strong>210 tests</strong> under consistent 
                    environmental conditions (white tabletop and lighting, in a <strong>zero-shot environment</strong>).
                </p>
                <p>
                    DexGraspVLA achieves <strong>over 98% success</strong> on both seen and unseen objects, significantly outperforming alternatives. 
                    Notably, it even performs slightly better on unseen objects, confirming that it generalizes beyond training data rather than overfitting. 
                    In contrast, baseline models struggle in the novel environment, as they directly map raw inputs to actions and are sensitive to perceptual shifts.
                </p>
            
            </div>
  </div>

  <h3 class="title is-4"><span
    class="dvima">Internal Model Behavior Analysis
  </span></h3>
  <div class="content has-text-justified">
          <img src="./assets/images/analysis.jpg" class="interpolation-image"
         alt="" style="display: block; margin-left: auto; margin-right: auto" width="600"/>
         <div class="content has-text-justified hover-card" style="padding: 1.5rem;">
            <p>
              We analyze the internal model behavior of DexGraspVLA to show that it <strong> is robust to environmental variations.</strong> The first row in the figure above presents the cropped raw head images of the same cluttered objects in four different environments: a white table, a calibration board, a tablecloth, and a tablecloth under disco light. The second row shows that the <strong>DINOv2 features</strong> of images are consistent across variations. The third row is the <strong>masks</strong> of the target objects accurately tracked by Cutie. The fourth row reflects that the averaged <strong>attention maps</strong> of DiT to head image features are also consistent regardless of perceptual differences. The fifth row confirms DexGraspVLA is attending to the correct object. Therefore, we substantiate that DexGraspVLA indeed transforms perceptually diverse raw inputs into invariant representations, on which it effectively applies imitation learning to model the data distribution, explaining its <strong>superior generalization performance</strong>.
            </p>
        
        </div>
</div>


<h3 class="title is-4"><span
  class="dvima">Long-Horizon Task Evaluation
</span></h3>
<div class="content has-text-justified">
        <img src="./assets/images/long-horizon.jpg" class="interpolation-image"
       alt="" style="display: block; margin-left: auto; margin-right: auto" width="600"/>
       <div class="content has-text-justified hover-card" style="padding: 1.5rem;">
          <p>
            To evaluate DexGraspVLA's capability on complex, long-horizon tasks, we design four user prompts: "Clear the table", "Grasp all bottles", "Grasp all green objects", and "Grasp all food". These prompts require common-sense and physical knowledge to identify appropriate grasping targets sequentially. For each prompt, we generate 24 cluttered scenes using unseen objects placed on a white tabletop under white lighting. Tasks involve grasping multiple relevant targets in sequence, requiring both high-level planning and precise low-level control.
          </p>
          <p>
            DexGraspVLA achieves an overall task success rate of <strong>89.6%</strong>, completing all stages of the multi-step prompts reliably. The high-level planner accurately proposes current grasping instructions (<strong>94.3% success</strong> on average), marks bounding boxes with over <strong>98% accuracy</strong>, and detects task completion to avoid redundant actions (<strong>94+% accuracy</strong>). The low-level controller executes grasps with over <strong>91% success</strong>, enabling robust step-by-step task execution. These results underscore DexGraspVLA's strong reasoning capability, perceptual grounding, and action reliability—working in synergy to complete long-horizon tasks without any task-specific training.
          </p>
      
      </div>
</div>

<h3 class="title is-4"><span
  class="dvima">Extended Application to Nonprehensile Grasping
</span></h3>
<div class="content has-text-justified">
        <img src="./assets/images/non-prehensile.jpg" class="interpolation-image"
       alt="" style="display: block; margin-left: auto; margin-right: auto" width="600"/>
       <div class="content has-text-justified hover-card" style="padding: 1.5rem;">
          <p>
            To test its generality, we extend DexGraspVLA to nonprehensile grasping, which requires object repositioning before lifting, by applying the same hierarchical framework without architectural changes. Trained on over 1,000 demonstrations with flat, hard-to-grasp objects, DexGraspVLA achieves 84.7% success on 144 tests with 18 unseen objects across novel lighting and background conditions. It learns to push objects toward the table edge before executing a stable grasp, showing robust generalization to shape, texture, and pose variations. This result highlights DexGraspVLA's flexibility in handling complex manipulation skills beyond dexterous grasping, and its generality in both perception and control.
          </p>

      </div>
</div>


    </div>
  </div>
</section>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <!-- Performance Demonstrations -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Performance Demonstrations</h2>
          <h3 class="title is-4"><span
            class="dvima">Dexterous Grasping in Unseen Cluttered Scenes
          </span></h3>
          <div class="container is-max-desktop">
            <div class="columns is-centered">
              <div class="column">
                <div class="columns is-centered">
                  <div class="column">
                    <video poster="" autoplay controls muted loop width="400">
                      <source src="assets/videos/normal/pen.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="column">
                    <video poster="" autoplay controls muted loop width="400">
                      <source src="assets/videos/normal/honey.mp4" type="video/mp4">
                    </video>
                  </div>
                </div>
                <div class="columns is-centered">
                  <div class="column">
                    <video poster="" autoplay controls muted loop width="400">
                      <source src="assets/videos/normal/milk-converted.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="column">
                    <video poster="" autoplay controls muted loop width="400">
                      <source src="assets/videos/normal/garbagebag-converted.mp4" type="video/mp4">
                    </video>
                  </div>
                </div>
              </div>
            </div>
          </div>
          <br>
          <br>
          <h3 class="title is-4"><span
            class="dvima">Lighting Generalization
          </span></h3>
          <div class="container is-max-desktop">
            <div class="columns is-centered">
              <div class="column">
                <div class="columns is-centered">
                  <div class="column">
                    <video poster="" autoplay controls muted loop width="400">
                      <source src="assets/videos/lighting/yogurt.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="column">
                    <video poster="" autoplay controls muted loop width="400">
                      <source src="assets/videos/lighting/sausage.mp4" type="video/mp4">
                    </video>
                  </div>
                </div>
                <div class="columns is-centered">
                  <div class="column">
                    <video poster="" autoplay controls muted loop width="400">
                      <source src="assets/videos/lighting/dish-cleaner.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="column">
                    <video poster="" autoplay controls muted loop width="400">
                      <source src="assets/videos/lighting/ya-potato.mp4" type="video/mp4">
                    </video>
                  </div>
                </div>
                <div class="columns is-centered">
                  <div class="column">
                    <video poster="" autoplay controls muted loop width="400">
                      <source src="assets/videos/lighting/yellow-green-box-converted.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="column">
                    <video poster="" autoplay controls muted loop width="400">
                      <source src="assets/videos/lighting/cola.mp4" type="video/mp4">
                    </video>
                  </div>
                </div>
                <div class="columns is-centered">
                  <div class="column">
                    <video poster="" autoplay controls muted loop width="400">
                      <source src="assets/videos/lighting/pepper-converted.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="column">
                    <video poster="" autoplay controls muted loop width="400">
                      <source src="assets/videos/lighting/anmuxi-yogurt-converted.mp4" type="video/mp4">
                    </video>
                  </div>
                </div>
              </div>
            </div>
          </div>
          <br>
          <br>
          <h3 class="title is-4"><span
            class="dvima">Background Generalization
          </span></h3>
          <div class="container is-max-desktop">
                <div class="columns is-centered">
                  <div class="column">
                    <video poster="" autoplay controls muted loop width="300">
                      <source src="assets/videos/background/2.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="column">
                    <video poster="" autoplay controls muted loop width="300">
                      <source src="assets/videos/background/3.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="column">
                    <video poster="" autoplay controls muted loop width="300">
                      <source src="assets/videos/background/4.mp4" type="video/mp4">
                    </video>
                  </div>
                </div>
                <div class="columns is-centered">
                  <div class="column">
                    <video poster="" autoplay controls muted loop width="300">
                      <source src="assets/videos/background/5.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="column">
                    <video poster="" autoplay controls muted loop width="300">
                      <source src="assets/videos/background/7.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="column">
                    <video poster="" autoplay controls muted loop width="300">
                      <source src="assets/videos/background/8.mp4" type="video/mp4">
                    </video>
                  </div>
                </div>
              </div>
          <br>
          <br>
          <h3 class="title is-4"><span
            class="dvima">Grasping Small Objects
          </span></h3>
          <div class="container is-max-desktop">
            <div class="columns is-centered">
              <div class="column">
                <div class="columns is-centered">
                  <div class="column">
                    <video poster="" autoplay controls muted loop width="400">
                      <source src="assets/videos/small-objects/thin-converted.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="column">
                    <video poster="" autoplay controls muted loop width="400">
                      <source src="assets/videos/small-objects/headphone-converted.mp4" type="video/mp4">
                    </video>
                  </div>
                </div>
                <div class="columns is-centered">
                  <div class="column">
                    <video poster="" autoplay controls muted loop width="400">
                      <source src="assets/videos/small-objects/teeth-converted.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="column">
                    <video poster="" autoplay controls muted loop width="400">
                      <source src="assets/videos/small-objects/earphones-converted.mp4" type="video/mp4">
                    </video>
                  </div>
                </div>
                </div>
              </div>
            </div>
            <br>
            <br>
            <h3 class="title is-4"><span
              class="dvima">Grasping Industry Objects
            </span></h3>
            <div class="container is-max-desktop">
              <div class="columns is-centered">
                <div class="column">
                  <div class="columns is-centered">
                    <div class="column">
                      <video poster="" autoplay controls muted loop width="400">
                        <source src="assets/videos/industry/1.mp4" type="video/mp4">
                      </video>
                    </div>
                    <div class="column">
                      <video poster="" autoplay controls muted loop width="400">
                        <source src="assets/videos/industry/2.mp4" type="video/mp4">
                      </video>
                    </div>
                  </div>
                  </div>
                </div>
              </div>
              <br>
              <br>
              <h3 class="title is-4"><span
                class="dvima">Re-grasps
              </span></h3>
              <div class="container is-max-desktop">
                    <div class="columns is-centered">
                      <div class="column">
                        <video poster="" autoplay controls muted loop width="600">
                          <source src="assets/videos/regrasp/chocolate-converted.mp4" type="video/mp4">
                        </video>
                      </div>
                    </div>
                    <div class="columns is-centered">
                      <div class="column">
                        <video poster="" autoplay controls muted loop width="600">
                          <source src="assets/videos/regrasp/clock-converted.mp4" type="video/mp4">
                        </video>
                      </div>
                    </div>
                    <div class="columns is-centered">
                      <div class="column">
                        <video poster="" autoplay controls muted loop width="600">
                          <source src="assets/videos/regrasp/lotion-converted.mp4" type="video/mp4">
                        </video>
                      </div>
                    </div>
                </div>
                <br>
                <br>
                <h3 class="title is-4"><span
                  class="dvima">Human Disturbance
                </span></h3>
                <div class="container is-max-desktop">
                  <div class="columns is-centered">
                    <div class="column">
                      <div class="columns is-centered">
                        <div class="column">
                          <video poster="" autoplay controls muted loop width="400">
                            <source src="assets/videos/human-intervention/2-tea-converted.mp4" type="video/mp4">
                          </video>
                        </div>
                        <div class="column">
                          <video poster="" autoplay controls muted loop width="400">
                            <source src="assets/videos/human-intervention/2-milk-converted.mp4" type="video/mp4">
                          </video>
                        </div>
                      </div>
                      <div class="columns is-centered">
                        <div class="column">
                          <video poster="" autoplay controls muted loop width="400">
                            <source src="assets/videos/human-intervention/1-milk-converted.mp4" type="video/mp4">
                          </video>
                        </div>
                        <div class="column">
                          <video poster="" autoplay controls muted loop width="400">
                            <source src="assets/videos/human-intervention/1-cola-converted.mp4" type="video/mp4">
                          </video>
                        </div>
                      </div>
                      </div>
                    </div>
                  </div>
                  <br>
                  <br>
                  <h3 class="title is-4"><span
                    class="dvima">Long-horizon Grasping
                  </span></h3>
                  <div class="container is-max-desktop">
                        <div class="columns is-centered">
                          <div class="column">
                            <video poster="" autoplay controls muted loop width="600">
                              <source src="assets/videos/long-horizon/long-horizon-no-voice.mp4" type="video/mp4">
                            </video>
                          </div>
                        </div>
                    </div>
                    <br>
                    <br>
                    <h3 class="title is-4"><span
                      class="dvima">Extended Application: Nonprehensile Grasping
                    </span></h3>
                    <div class="content has-text-centered">
                      <p style="font-size: 120%">
                        To evaluate its generality, we extend DexGraspVLA to <strong>nonprehensile grasping</strong>, where the robot first repositions the nonprehensile object toward the table edge before executing a stable grasp. The controller is trained on 1,029 human demonstrations. Below are some deployment results.
                      </p>
                    </div>
                    <div class="container is-max-desktop">
                      <div class="columns is-centered">
                        <div class="column">
                          <div class="columns is-centered">
                            <div class="column">
                              <video poster="" autoplay controls muted loop width="400">
                                <source src="assets/videos/nonprehensile/spaghetti.mp4" type="video/mp4">
                              </video>
                            </div>
                            <div class="column">
                              <video poster="" autoplay controls muted loop width="400">
                                <source src="assets/videos/nonprehensile/calculus.mp4" type="video/mp4">
                              </video>
                            </div>
                          </div>
                          <div class="columns is-centered">
                            <div class="column">
                              <video poster="" autoplay controls muted loop width="400">
                                <source src="assets/videos/nonprehensile/chopstick.mp4" type="video/mp4">
                              </video>
                            </div>
                            <div class="column">
                              <video poster="" autoplay controls muted loop width="400">
                                <source src="assets/videos/nonprehensile/laver.mp4" type="video/mp4">
                              </video>
                            </div>
                          </div>
                          <div class="columns is-centered">
                            <div class="column">
                              <video poster="" autoplay controls muted loop width="400">
                                <source src="assets/videos/nonprehensile/towel.mp4" type="video/mp4">
                              </video>
                            </div>
                            <div class="column">
                              <video poster="" autoplay controls muted loop width="400">
                                <source src="assets/videos/nonprehensile/plate.mp4" type="video/mp4">
                              </video>
                            </div>
                          </div>
                          <div class="columns is-centered">
                            <div class="column">
                              <video poster="" autoplay controls muted loop width="400">
                                <source src="assets/videos/nonprehensile/disco.mp4" type="video/mp4">
                              </video>
                            </div>
                            <div class="column">
                              <video poster="" autoplay controls muted loop width="400">
                                <source src="assets/videos/nonprehensile/disco-2.mp4" type="video/mp4">
                              </video>
                            </div>
                          </div>
                        </div>
                      </div>
                    </div>
                  <br>
                  <br>
                  <h3 class="title is-4"><span
                    class="dvima">Robot Shakes Hand with Human
                  </span></h3>
                  <div class="container is-max-desktop">
                        <div class="columns is-centered">
                          <div class="column">
                            <video poster="" autoplay controls muted loop width="600">
                              <source src="assets/videos/shake-hand/shakehand.mp4" type="video/mp4">
                            </video>
                          </div>
                        </div>
                    </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <!-- Generalization Videos -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Conclusion</h2>
      <div class="column">
          <div class="content has-text-justified">
          <p style="font-size: 125%">
            We present DexGraspVLA, a hierarchical VLA framework for general-purpose dexterous grasping. By leveraging a pre-trained VLM as the high-level planner and a diffusion-based low-level controller, the system transforms diverse multimodal inputs into domain-invariant representations and learns robust closed-loop grasping policies via imitation learning. Our large-scale evaluations show over 90% success across thousands of unseen cluttered scenes in a zero-shot setting, with empirical evidence of strong generalization and consistent internal behavior. DexGraspVLA also handles free-form long-horizon prompts, recovers from failures, and extends to nonprehensile grasping, demonstrating broad applicability.
        </p>

        </div>
      </div>

    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p> Website borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> under a <a
              href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
